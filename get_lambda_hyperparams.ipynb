{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loop over lambda:\n",
    "  • Find best NN hyperparams (n_hidden, n_layers, dropout, lr) via GroupKFold CV on ALL isotopes\n",
    "    (λ is fixed)\n",
    "  • Then do leave-one-out over isotopes: train on N−1, test on the held-out. \n",
    "    Save metrics + plots.\n",
    "  • Write a summary.csv of per-isotope metrics (and best params) in each folder.\n",
    "\n",
    "  Run the cells below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install pandas numpy scikit-learn torch optuna matplotlib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import random, json, math, pathlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch, torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, Sampler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GroupKFold\n",
    "import optuna\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ----------------------------- CONFIG ---------------------------------\n",
    "CSV_PATH   = \"merged_8_july_d_lowmassremoved.csv\"\n",
    "DEVICE     = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "EPS        = 1e-12\n",
    "SEED       = 42\n",
    "TRIALS     = 1\n",
    "N_SPLITS   = 5\n",
    "MAX_EPOCHS = 200\n",
    "PATIENCE   = 15\n",
    "\n",
    "FEATURES = [\n",
    "    'ERG','Radius','n_rms_radius','p_rms_radius','octopole_deformation',\n",
    "    'n_chem_erg','Sn','Z','S2p_compound','gamma_deformation','ME',\n",
    "    'BEA_A_daughter','A','Radius_daughter','BEA_A','AM','Radius_compound',\n",
    "    'Pairing_daughter','BEA_compound','beta_deformation','Theory_thresh',\n",
    "    'shell_P','pairing_delta','symmetry_S','excitation_energy',\n",
    "    'Thresh_n3n','Excite_n3n','Thresh_n4n','Excite_n4n'\n",
    "]\n",
    "TARGET, SIGMA_EXP, FLUENCE, GAMMA, ISO = (\n",
    "    \"XS\",\"sigma_exp\",\"fluence\",\"gamma\",\"iso_id\"\n",
    ")\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "\n",
    "# ----------------------- DATASET & SAMPLER ----------------------------\n",
    "class XSData(Dataset):\n",
    "    def __init__(self, df, scaler:StandardScaler, fit_scaler=False):\n",
    "        if fit_scaler:\n",
    "            scaler.fit(df[FEATURES])\n",
    "        self.X = torch.tensor(scaler.transform(df[FEATURES]).astype(np.float32))\n",
    "        self.y = torch.tensor(df[TARGET].to_numpy(np.float32))\n",
    "        self.fluence   = torch.tensor(df[FLUENCE].to_numpy(np.float32))\n",
    "        self.sigma_exp = torch.tensor(df[SIGMA_EXP].fillna(0).to_numpy(np.float32))\n",
    "        self.gamma     = torch.tensor(df[GAMMA].to_numpy(np.float32))\n",
    "        self.iso_code  = df[ISO].astype(\"category\").cat.codes.to_numpy()\n",
    "\n",
    "    def __len__(self): return len(self.y)\n",
    "    def __getitem__(self,i):\n",
    "        return {\n",
    "            \"x\": self.X[i],\n",
    "            \"y\": self.y[i],\n",
    "            \"fluence\": self.fluence[i],\n",
    "            \"sigma_exp\": self.sigma_exp[i],\n",
    "            \"gamma\": self.gamma[i],\n",
    "            \"iso_code\": self.iso_code[i]\n",
    "        }\n",
    "\n",
    "class IsoBatchSampler(Sampler):\n",
    "    \"\"\"One whole isotope per batch.\"\"\"\n",
    "    def __init__(self, iso_codes, shuffle=True):\n",
    "        self.ids = {}\n",
    "        for idx, iso in enumerate(iso_codes):\n",
    "            self.ids.setdefault(int(iso), []).append(idx)\n",
    "        self.keys = list(self.ids.keys())\n",
    "        self.shuffle = shuffle\n",
    "\n",
    "    def __iter__(self):\n",
    "        keys = self.keys.copy()\n",
    "        if self.shuffle: random.shuffle(keys)\n",
    "        for k in keys:\n",
    "            yield self.ids[k]\n",
    "\n",
    "    def __len__(self): return len(self.keys)\n",
    "\n",
    "\n",
    "# ------------------------------ MODEL ---------------------------------\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, n_in, n_hidden, n_layers, p_drop):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        for l in range(n_layers):\n",
    "            layers += [\n",
    "                nn.Linear(n_in if l==0 else n_hidden, n_hidden),\n",
    "                nn.ReLU(), nn.Dropout(p_drop)\n",
    "            ]\n",
    "        layers += [nn.Linear(n_hidden,1)]\n",
    "        self.net = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self,x): return self.net(x).squeeze(1)\n",
    "\n",
    "\n",
    "# -------------------------- CUSTOM LOSS -------------------------------\n",
    "def iso_loss(batch, y_hat, lam):\n",
    "    mse = nn.functional.mse_loss(y_hat, batch[\"y\"])\n",
    "    if batch[\"gamma\"][0] == 0:\n",
    "        return mse\n",
    "    flu        = batch[\"fluence\"]\n",
    "    sigma_pred = (flu * y_hat).sum() / (flu.sum() + EPS)\n",
    "    sigma_true = batch[\"sigma_exp\"][0]\n",
    "    rel_err_sq = ((sigma_pred - sigma_true)/(sigma_true+EPS))**2\n",
    "    return mse + lam * rel_err_sq\n",
    "\n",
    "\n",
    "# ------------------------ TRAIN / VALID LOOP --------------------------\n",
    "def run_epoch(model, loader, opt, lam, train=True):\n",
    "    model.train(mode=train)\n",
    "    total, n = 0.0, 0\n",
    "    for batch in loader:\n",
    "        batch = {k:(v.to(DEVICE) if torch.is_tensor(v) else v)\n",
    "                 for k,v in batch.items()}\n",
    "        if train: opt.zero_grad(set_to_none=True)\n",
    "        preds = model(batch[\"x\"])\n",
    "        loss  = iso_loss(batch, preds, lam)\n",
    "        if train:\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "        total += loss.item() * len(preds)\n",
    "        n += len(preds)\n",
    "    return total / n\n",
    "\n",
    "\n",
    "# --------------------- OPTUNA OBJECTIVE -------------------------------\n",
    "def make_objective(df, lam):\n",
    "    def objective(trial):\n",
    "        hp = dict(\n",
    "            n_hidden = trial.suggest_int(\"n_hidden\", 32,256,32),\n",
    "            n_layers = trial.suggest_int(\"n_layers\", 1,4),\n",
    "            dropout  = trial.suggest_float(\"dropout\",0.0,0.5),\n",
    "            lr       = trial.suggest_loguniform(\"lr\",1e-4,1e-2)\n",
    "        )\n",
    "        gkf = GroupKFold(n_splits=N_SPLITS)\n",
    "        cv_losses = []\n",
    "        for tr_idx, va_idx in gkf.split(df, groups=df[ISO]):\n",
    "            scaler = StandardScaler()\n",
    "            ds_tr = XSData(df.iloc[tr_idx], scaler, fit_scaler=True)\n",
    "            ds_va = XSData(df.iloc[va_idx], scaler, fit_scaler=False)\n",
    "            dl_tr = DataLoader(ds_tr, batch_sampler=IsoBatchSampler(ds_tr.iso_code))\n",
    "            dl_va = DataLoader(ds_va, batch_sampler=IsoBatchSampler(ds_va.iso_code,shuffle=False))\n",
    "            model = MLP(len(FEATURES), hp[\"n_hidden\"], hp[\"n_layers\"], hp[\"dropout\"]).to(DEVICE)\n",
    "            opt   = torch.optim.Adam(model.parameters(), lr=hp[\"lr\"])\n",
    "            best_va = float(\"inf\"); patience=0\n",
    "            for ep in range(MAX_EPOCHS):\n",
    "                run_epoch(model, dl_tr, opt, lam, train=True)\n",
    "                v = run_epoch(model, dl_va, opt, lam, train=False)\n",
    "                if v < best_va:\n",
    "                    best_va = v; patience = 0\n",
    "                else:\n",
    "                    patience += 1\n",
    "                    if patience >= PATIENCE:\n",
    "                        break\n",
    "            cv_losses.append(best_va)\n",
    "        return np.mean(cv_losses)\n",
    "    return objective\n",
    "\n",
    "\n",
    "# ------------------------------- MAIN ---------------------------------\n",
    "if __name__==\"__main__\":\n",
    "    df_all = pd.read_csv(CSV_PATH)\n",
    "    iso_list = df_all[ISO].unique()\n",
    "\n",
    "    for lam in np.round(np.arange(0,1.01,0.1),1):\n",
    "        # 1) prepare folder\n",
    "        out = pathlib.Path(f\"results_lam{lam:.1f}\")\n",
    "        out.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        # 2) hyperparam search on ALL isotopes\n",
    "        study = optuna.create_study(direction=\"minimize\",\n",
    "                                   study_name=f\"lam{lam:.1f}\",\n",
    "                                   storage=f\"sqlite:///{out/'study.db'}\",\n",
    "                                   load_if_exists=True)\n",
    "        study.optimize(make_objective(df_all, lam),\n",
    "                       n_trials=TRIALS, show_progress_bar=True)\n",
    "        best = study.best_trial.params\n",
    "        # save best params\n",
    "        with open(out/\"best_params.json\",\"w\") as fh:\n",
    "            json.dump({\"lam\":lam,**best}, fh, indent=2)\n",
    "\n",
    "        # 3) leave-one-out test\n",
    "        records = []\n",
    "        for iso in iso_list:\n",
    "            df_tr = df_all[df_all[ISO]!=iso].reset_index(drop=True)\n",
    "            df_te = df_all[df_all[ISO]==iso].reset_index(drop=True)\n",
    "\n",
    "            # train on df_tr\n",
    "            scaler = StandardScaler()\n",
    "            ds_tr = XSData(df_tr, scaler, fit_scaler=True)\n",
    "            dl_tr = DataLoader(ds_tr, batch_sampler=IsoBatchSampler(ds_tr.iso_code))\n",
    "            model = MLP(len(FEATURES),\n",
    "                        best[\"n_hidden\"], best[\"n_layers\"], best[\"dropout\"]).to(DEVICE)\n",
    "            opt   = torch.optim.Adam(model.parameters(), lr=best[\"lr\"])\n",
    "            for ep in range(MAX_EPOCHS):\n",
    "                _ = run_epoch(model, dl_tr, opt, lam, train=True)\n",
    "\n",
    "            # eval on df_te\n",
    "            ds_te = XSData(df_te, scaler, fit_scaler=False)\n",
    "            dl_te = DataLoader(ds_te, batch_sampler=IsoBatchSampler(ds_te.iso_code,shuffle=False))\n",
    "            model.eval()\n",
    "            preds, flu, sigma_true = [], None, None\n",
    "            with torch.no_grad():\n",
    "                for b in dl_te:\n",
    "                    b = {k:(v.to(DEVICE) if torch.is_tensor(v) else v) \n",
    "                         for k,v in b.items()}\n",
    "                    outp = model(b[\"x\"]).cpu()\n",
    "                    preds.append(outp)\n",
    "                    flu = b[\"fluence\"].cpu()\n",
    "                    sigma_true = float(b[\"sigma_exp\"][0].cpu())\n",
    "            preds = torch.cat(preds)\n",
    "            mse = float(nn.functional.mse_loss(preds, ds_te.y).cpu())\n",
    "            sigma_pred = float((flu*preds).sum() / (flu.sum()+EPS))\n",
    "            rel_err = (sigma_pred - sigma_true)/(sigma_true+EPS)\n",
    "\n",
    "            # save per-iso plots\n",
    "            # 3a) scatter True vs Pred\n",
    "            plt.figure(figsize=(4,4))\n",
    "            plt.scatter(ds_te.y, preds, s=10)\n",
    "            mn, mx = min(ds_te.y.min(), preds.min()), max(ds_te.y.max(), preds.max())\n",
    "            plt.plot([mn,mx],[mn,mx],\"--\",lw=1)\n",
    "            plt.xlabel(\"True XS\"); plt.ylabel(\"Pred XS\")\n",
    "            plt.title(f\"{iso} XS scatter\")\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(out/f\"{iso}_scatter.png\")\n",
    "            plt.close()\n",
    "\n",
    "            # 3b) ERG vs curves\n",
    "            plt.figure()\n",
    "            plt.plot(df_te[\"ERG\"], df_te[\"XS\"],    label=\"True\")\n",
    "            plt.plot(df_te[\"ERG\"], preds.numpy(),  label=\"Pred\")\n",
    "            plt.xlabel(\"ERG\"); plt.ylabel(\"XS\")\n",
    "            plt.title(f\"{iso} ERG vs XS\")\n",
    "            plt.legend()\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(out/f\"{iso}_erg_xs.png\")\n",
    "            plt.close()\n",
    "\n",
    "            # record metrics\n",
    "            rec = {\n",
    "                \"iso_id\": iso,\n",
    "                \"mse\": mse,\n",
    "                \"sigma1g_pred\": sigma_pred,\n",
    "                \"sigma1g_true\": sigma_true,\n",
    "                \"rel_err\": rel_err,\n",
    "                **best, \"lam\": lam\n",
    "            }\n",
    "            records.append(rec)\n",
    "\n",
    "        # 4) summary CSV\n",
    "        pd.DataFrame(records).to_csv(out/\"summary.csv\", index=False)\n",
    "        print(f\"Done λ={lam:.1f} → folder: {out}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
